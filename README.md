# Hi, I'm Jean ğŸ‘‹

**AI/LLM Security Engineer | Red Team for AI Systems | AI Risk Assessment & Insurability**

I specialize in adversarial testing of AI systems and translating AI security risks into 
business-critical governance frameworks. I build attack chains for LLMs, design guardrail 
bypass techniques, and create risk assessment frameworks that determine whether AI systems 
are safe enough to insure, deploy, and trust.

Currently: [Your current role/status] | [Certifications if any]

ğŸ”— [LinkedIn] | ğŸ“ [Blog/Portfolio] | ğŸ¯ [AI-LLM Security Lab](https://github.com/Jeanmatozo/AI-LLM-security-lab)
```
```
### **ğŸ’¼ Core Competencies**

**AI/LLM Red Teaming & Adversarial Testing**
- Prompt injection & jailbreak techniques
- Model guardrail bypass strategies  
- Indirect prompt injection via RAG poisoning
- Multi-step attack chain development
- Tool misuse & privilege escalation in agents

**AI Risk Assessment & Insurability Engineering**
- Risk scoring frameworks for AI systems
- Security posture evaluation for LLMs
- Compliance mapping (NIST AI RMF, ISO 42001)
- Pre-deployment security assessments
- Insurance readiness evaluations

**AI Governance & Auditability**
- Policy-as-code for AI guardrails
- Automated compliance checking
- Audit logging for AI decisions
- Explainability frameworks for black-box models
- Regulatory alignment (EU AI Act, GDPR AI clauses)

**Agentic Systems Security**
- Multi-agent orchestration vulnerabilities
- Tool-calling authorization frameworks
- Agent memory poisoning detection
- Long-context manipulation attacks
- Autonomous system containment strategies

---


