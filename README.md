# Hi, I'm Jean ğŸ‘‹

**AI/LLM Security Engineer | Red Team for AI Systems | AI Risk Assessment & Insurability**

I specialize in the adversarial testing of AI systems and the translation of technical AI security risks into business-critical governance frameworks. My work focuses on building attack chains for Large Language Models (LLMs), designing sophisticated guardrail bypass techniques, and creating risk assessment frameworks that determine whether AI systems are safe enough to insure, deploy, and trust.

Currently: AI Security Researcher & Practitioner | AI/LLM Red Teaming | NIST AI RMF & ISO/IEC 27001

ğŸ”— [LinkedIn](https://www.linkedin.com/in/jean-akingeneye-00500213/) | ğŸ“ [Technical Blog](#) | ğŸ¯ [AI-LLM Security Lab](https://github.com/Jeanmatozo/AI-LLM-security-lab)

---

## ğŸ¯ What I'm Working On

*   ğŸ›¡ï¸ Developing **automated red teaming frameworks** for multi-agent systems.
*   ğŸ” Researching **indirect prompt injection** vulnerabilities in RAG-based architectures.
*   ğŸ“Š Building a **quantitative AI risk scoring engine** for insurance readiness.
*   ğŸŒ± Exploring **policy-as-code** for real-time LLM guardrail enforcement.

---

## ğŸ’¼ Core Competencies

### **AI/LLM Red Teaming & Adversarial Testing**
I conduct deep-dive security assessments of AI models, focusing on prompt injection, jailbreaking, and model guardrail bypass strategies. My expertise includes developing multi-step attack chains and identifying vulnerabilities in tool-calling and agentic memory systems.

### **AI Risk Assessment & Insurability Engineering**
I bridge the gap between technical vulnerabilities and business risk. I develop scoring frameworks based on NIST AI RMF and ISO 42001 to evaluate the security posture of LLMs, providing actionable insights for pre-deployment and insurance readiness.

### **AI Governance & Auditability**
I implement policy-driven security architectures, using policy-as-code to enforce guardrails and ensure regulatory alignment with frameworks like the EU AI Act. My work includes creating automated compliance checking and explainability frameworks for black-box models.

---

## ğŸš€ Featured Projects

### AI Security & Red Teaming

| | |
| --- | --- |
| **ğŸ”´ LLM Red Team Toolkit**<br>A comprehensive collection of prompt injection attacks, jailbreak templates, and automated evaluation scripts for testing model robustness.<br>[View Project â†’](#) | **ğŸ›¡ï¸ Guardrail Bypass Laboratory**<br>Systematic testing and comparative analysis of commercial AI safety systems to identify and document bypass techniques.<br>[View Project â†’](#) |
| **ğŸ¤– Agentic Security Sandbox**<br>A vulnerable-by-design AI agent environment for security training, featuring scenarios for tool misuse and privilege escalation.<br>[View Project â†’](#) | **ğŸ“š RAG Security Audit Framework**<br>Testing framework for Retrieval-Augmented Generation systems, focusing on document poisoning and context injection.<br>[View Project â†’](#) |

### AI Governance & Risk Management

| Project | Technology Stack | Description |
| --- | --- | --- |
| [AI Risk Assessment Template](#) | Python, YAML, NIST AI RMF | Standardized framework for evaluating AI system security and insurability. |
| [AI Governance-as-Code](#) | Python, OPA, Rego | Policy enforcement engine for real-time LLM output filtering and compliance. |
| [Insurability Verdict Generator](#) | FastAPI, React, SQL | Mock platform for calculating AI insurance premiums based on security posture. |

---

## ğŸ› ï¸ Technology Stack

**AI/ML Frameworks**  
![LangChain](https://img.shields.io/badge/LangChain-1C3C3C?style=for-the-badge&logo=langchain&logoColor=white) ![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white) ![Anthropic](https://img.shields.io/badge/Anthropic-755139?style=for-the-badge) ![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-FFD21E?style=for-the-badge)

**Security & Red Teaming**  
![Burp Suite](https://img.shields.io/badge/Burp%20Suite-FF6633?style=for-the-badge&logo=burp-suite&logoColor=white) ![OWASP](https://img.shields.io/badge/OWASP-373A3C?style=for-the-badge&logo=owasp&logoColor=white) ![Metasploit](https://img.shields.io/badge/Metasploit-2496ED?style=for-the-badge)

**Infrastructure & Automation**  
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white) ![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white) ![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-2088FF?style=for-the-badge&logo=github-actions&logoColor=white) ![Terraform](https://img.shields.io/badge/Terraform-7B42BC?style=for-the-badge&logo=terraform&logoColor=white)

---

## ğŸ“ˆ Security Research Metrics

*   ğŸ”´ **150+** Documented prompt injection variants
*   ğŸ¯ **15** AI systems tested for vulnerabilities
*   ğŸ“Š **8** Risk assessment frameworks developed
*   ğŸ›¡ï¸ **95%** Guardrail bypass success rate (controlled testing)

---

## ğŸ“ Community & Thought Leadership

*   ğŸ“ **AI Security Blog** - Weekly writeups on LLM vulnerabilities and defense strategies.
*   ğŸ¤ **Conference Talks** - Presenting on "The Future of AI Insurability" at local security meetups.
*   ğŸ¤ **Open Source** - Contributor to AI security evaluation benchmarks.
*   ğŸ’¡ **CTF Creator** - Designing AI-focused security challenges for the community.

---

## ğŸ“« Let's Connect

I am always open to discussing AI security research, adversarial testing methodologies, and the evolution of AI governance.

_"Securing the future of intelligence by thinking like the adversary today."_
