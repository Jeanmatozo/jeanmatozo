# Hi, I'm Jean üëã

**AI/LLM Security Engineer | AI Red Teaming & Risk Assessment | Governance-Driven AI Security**

I specialize in the adversarial testing of AI systems and translating technical AI security risks into business-critical governance frameworks. My work focuses on building attack chains for Large Language Models (LLMs), designing guardrail bypass techniques, and creating AI risk assessment frameworks and testing artifacts that help organizations evaluate whether AI systems are safe enough to deploy, govern, and eventually insure.

**Currently:** AI Security Researcher & Practitioner focused on AI/LLM red teaming, risk assessment, and governance, aligned with **NIST AI RMF** and **ISO/IEC 27001**.

üîó [LinkedIn](https://www.linkedin.com/in/jean-akingeneye-00500213/) |  [Technical Blog](#) |  [AI-LLM Security Lab](https://github.com/Jeanmatozo/AI-LLM-security-lab)

---

## What I'm Working On

- Developing **automated red teaming frameworks** for multi-agent systems  
- Researching **indirect prompt injection** vulnerabilities in RAG-based architectures  
- Building a **quantitative AI risk scoring engine** to explore deployment and insurance readiness  
- Experimenting with **policy-as-code** approaches for real-time LLM guardrail enforcement  

---

## Core Competencies

### **AI/LLM Red Teaming & Adversarial Testing**
I design and execute research-driven security assessments of AI models, focusing on prompt injection, jailbreaking, and guardrail bypass techniques. My work includes developing multi-step attack chains and identifying vulnerabilities in tool-calling and agentic memory systems.

### **AI Risk Assessment & Insurability Engineering**
I bridge the gap between technical vulnerabilities and business risk by developing scoring frameworks aligned with **NIST AI RMF** and **ISO/IEC 42001**. These frameworks provide actionable, framework-mapped insights to support pre-deployment reviews and future insurance readiness discussions.

### **AI Governance & Auditability**
I implement policy-driven security architectures using policy-as-code to enforce guardrails and support regulatory alignment with emerging frameworks such as the **EU AI Act**. My work includes automated compliance checks and explainability approaches for black-box models.

---

## Featured Projects

### üî¨ AI-LLM Security Lab *(Core Research Program)*
A hands-on security research lab focused on **adversarial testing of AI systems**, covering prompt injection, RAG security, agentic tool abuse, and AI risk assessment.  
This lab serves as the **umbrella environment** where attacks, defenses, and governance frameworks are designed, tested, and documented.

[View Lab ‚Üí](https://github.com/Jeanmatozo/AI-LLM-security-lab)

---

### AI Security & Red Teaming

| | |
| --- | --- |
| **üî¥ LLM Red Team Toolkit** *(Research / Active Development)*<br>Adversary-focused red teaming of LLMs using realistic attack flows, emphasizing reasoning, sequencing, and boundary discovery.<br>[View Project ‚Üí](https://github.com/Jeanmatozo/llm-red-team-toolkit/tree/main) | **üõ°Ô∏è Guardrail Bypass Laboratory** *(Comparative Testing)*<br>Systematic testing and comparative analysis of commercial AI safety systems to identify and document bypass techniques.<br>[View Project ‚Üí](#) |
| **ü§ñ Agentic Security Sandbox** *(Training & Experimentation)*<br>A vulnerable-by-design AI agent environment featuring scenarios for tool misuse and privilege escalation.<br>[View Project ‚Üí](#) | **üìö RAG Security Audit Framework** *(Framework & Testing)*<br>Testing framework for Retrieval-Augmented Generation systems, focusing on document poisoning and context injection.<br>[View Project ‚Üí](#) |

---

### AI Governance & Risk Management

| Project | Technology Stack | Description |
| --- | --- | --- |
| **AI Risk Assessment Template** *(Prototype / Framework Exploration)* | Python, YAML, NIST AI RMF | Standardized framework for evaluating AI system security and supporting deployment and future insurability decisions. |
| **AI Governance-as-Code** *(Experimental)* | Python, OPA, Rego | Policy-as-code patterns for enforcing guardrails and compliance controls in AI systems. |
| **Insurability Verdict Generator** *(Concept / Mock Platform)* | FastAPI, React, SQL | Conceptual platform for exploring how AI security posture could inform insurance and deployment decisions. |

---

## Technology Stack

**AI/ML Frameworks**  
![LangChain](https://img.shields.io/badge/LangChain-1C3C3C?style=for-the-badge&logo=langchain&logoColor=white) ![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white) ![Anthropic](https://img.shields.io/badge/Anthropic-755139?style=for-the-badge) ![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-FFD21E?style=for-the-badge)

**Security & Red Teaming**  
![Burp Suite](https://img.shields.io/badge/Burp%20Suite-FF6633?style=for-the-badge&logo=burp-suite&logoColor=white) ![OWASP](https://img.shields.io/badge/OWASP-373A3C?style=for-the-badge&logo=owasp&logoColor=white) ![Metasploit](https://img.shields.io/badge/Metasploit-2496ED?style=for-the-badge)

**Infrastructure & Automation**  
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white) ![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white) ![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-2088FF?style=for-the-badge&logo=github-actions&logoColor=white) ![Terraform](https://img.shields.io/badge/Terraform-7B42BC?style=for-the-badge&logo=terraform&logoColor=white)

---

## üìà Security Research Metrics *(Lab-Based)*

- **150+** documented prompt injection variants  
- **15** AI systems tested in controlled environments  
- **8** AI risk assessment frameworks developed  
- **95%** guardrail bypass success rate *(controlled lab testing)*  

---

## Community & Thought Leadership

- **Cyber Touch Point Community** - Contributor & technical participant
- **CMMC professionals network (cpn)** - Member and mentee
- **LLM Security and AI Red Teaming group** - Member

---

##  Let‚Äôs Connect

I am always open to discussing AI security research, adversarial testing methodologies, and the evolving landscape of AI governance and risk.

*‚ÄúSecuring the future of intelligence by thinking like the adversary.‚Äù*
