# Hi, I'm Jean üëã

**AI/LLM Security Engineer | AI Red Teaming & Risk Assessment | Governance-Driven AI Security**

I specialize in the adversarial testing of AI systems and translating technical AI security risks into business-critical governance frameworks. My work focuses on building attack chains for Large Language Models (LLMs), designing guardrail bypass techniques, and creating AI risk assessment frameworks and testing artifacts that help organizations evaluate whether AI systems are safe enough to deploy, govern, and eventually insure.

**Currently:** AI Security Researcher & Practitioner focused on AI/LLM red teaming, risk assessment, and governance, aligned with **NIST AI RMF** and **ISO/IEC 27001**.

üîó [LinkedIn](https://www.linkedin.com/in/jean-akingeneye-00500213/) |  [Technical Blog](#) |  [AI-LLM Security Lab](https://github.com/Jeanmatozo/AI-LLM-security-lab)

---

## What I'm Working On

- Developing **adversary-driven red teaming frameworks** for LLM and agentic systems  
- Researching **RAG and indirect prompt injection risks** that impact enterprise deployment  
- Building **governance-aligned AI risk scoring artifacts** to support deployment and insurance readiness

---

## Core Competencies

### **AI/LLM Red Teaming & Adversarial Testing**
I design and execute research-driven security assessments of AI models, focusing on prompt injection, jailbreaking, and guardrail bypass techniques with an emphasis on multi-step attack chains, tool misuse, and boundary expansion

### **AI Risk Assessment & Insurability Engineering**
I bridge the gap between technical vulnerabilities and business risk by developing scoring frameworks aligned with **NIST AI RMF** and **ISO/IEC 42001**. These frameworks provide actionable, framework-mapped insights to support pre-deployment reviews and future insurance readiness discussions.

### **AI Governance & Auditability**
I implement policy-driven security architectures using policy-as-code to enforce guardrails and support regulatory alignment with emerging frameworks such as the **EU AI Act**. My work includes automated compliance checks and explainability approaches for black-box models.

---

## Featured Projects

### üî¨ AI-LLM Security Lab *(Core Research Program)*
A hands-on security research lab focused on **adversarial testing of AI systems**, covering prompt injection, RAG security, agentic tool abuse, and AI risk assessment.  
This lab serves as the **umbrella environment** where attacks, defenses, and governance frameworks are designed, tested, and documented.

[View Lab ‚Üí](https://github.com/Jeanmatozo/AI-LLM-security-lab)

---

### AI Security & Red Teaming

| | |
| --- | --- |
| **üî¥ LLM Red Team Toolkit** *(Research / Active Development)*<br>Adversary-focused red teaming of LLMs using realistic attack flows, emphasizing reasoning, sequencing, and boundary discovery.<br>[View Project ‚Üí](https://github.com/Jeanmatozo/llm-red-team-toolkit/tree/main) | **üõ°Ô∏è AI Guardrail Evaluation Lab** *(Comparative Assurance)*<br>A comparative testing framework for evaluating the effectiveness and failure modes of commercial AI guardrails, focused on governance, deployment risk, and assurance use cases.<br>[View Project ‚Üí](#) |
| **üìö RAG Security Risk Assessment Framework** <br>A governance-oriented testing framework for Retrieval-Augmented Generation (RAG) systems, analyzing document poisoning, context injection, and retrieval boundary failures.<br>[View Project ‚Üí](#) |

---

### AI Governance & Risk Engineering

In addition to adversarial testing, I develop **governance-aligned risk artifacts**‚Äîincluding AI risk assessment templates, policy-as-code patterns, and deployment readiness signals‚Äîto help organizations interpret technical findings through the lens of **NIST AI RMF, ISO/IEC standards, and emerging regulatory frameworks**.

---

## Technology Stack 

**AI/ML Frameworks**  
![LangChain](https://img.shields.io/badge/LangChain-1C3C3C?style=for-the-badge&logo=langchain&logoColor=white) ![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white) ![Anthropic](https://img.shields.io/badge/Anthropic-755139?style=for-the-badge) ![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-FFD21E?style=for-the-badge)

**Adversarial Testing & Evaluation**  
![OWASP](https://img.shields.io/badge/OWASP-373A3C?style=for-the-badge&logo=owasp&logoColor=white)

**Infrastructure & Automation**  
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-2088FF?style=for-the-badge&logo=github-actions&logoColor=white)

**Data & Artifacts**  
JSON / JSON Schema, YAML, audit-ready evaluation artifacts, reproducible test outputs for governance and review.

---

**Governance & Policy**  
NIST AI RMF, ISO/IEC frameworks (27001, 42001), policy-as-code, runtime guardrail enforcement, compliance-aligned risk translation.

OPA (Rego) for policy evaluation and enforcement prototypes.

---

## Research Signals *(Controlled Lab Environments)*

- Documented adversarial prompt patterns and attack variants
- Multiple AI systems evaluated using repeatable test harnesses
- Governance-aligned risk assessment artifacts developed for comparative analysis

---

## Community & Thought Leadership

- **Cyber Touch Point Community** - Contributor & technical participant
- **CMMC professionals network (cpn)** - Member and mentee
- **LLM Security and AI Red Teaming group** - Member

---

##  Let‚Äôs Connect

I am always open to discussing AI security research, adversarial testing methodologies, and the evolving landscape of AI governance and risk.

*‚ÄúSecuring the future of intelligence by thinking like the adversary.‚Äù*
